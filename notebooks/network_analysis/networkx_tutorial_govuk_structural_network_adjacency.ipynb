{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOV.UK structural network adjacency\n",
    "We work through a tutorial to learn about key network metrics and statistics based on this [blog post](https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python#fnref:import). \n",
    "\n",
    "We will be using the [GOV.UK structural network adjacency list](https://ckan.publishing.service.gov.uk/dataset/gov-uk-structural-network-adjacency-list?utm_source=ckan) data for this tutorial. This shows the connections (edges) between source and sink pages (nodes).\n",
    "\n",
    "This tutorial will take us through the basics of network visualisations and analysis using NetworkX and Python visualisation tools. The data itself doesn't matter so much, we are learning the principles of graph theory and `networkx` functionality.\n",
    "\n",
    "For further ideas see the [list of algorithms](https://networkx.github.io/documentation/stable/reference/algorithms/index.html) available for use with networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import community\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from holoviews.element.graphs import layout_nodes\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd\n",
    "\n",
    "# Enable multiline outputs in this notebook\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Enable interactivity for plots\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "Make sure to [download the data]((https://ckan.publishing.service.gov.uk/dataset/gov-uk-structural-network-adjacency-list?utm_source=ckan)), and place it in the `data/processed_network` folder at the parent-level of this repository.\n",
    "\n",
    "Rather than use the method of reading in the data used in the Quaker tutorial, we will opt for an approach using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a relative path to the data\n",
    "input_dir = os.path.join(\"..\", \"..\", \"data\", \"processed_network\")\n",
    "data_path = os.path.join(input_dir, \"structural_network_adjacency_list_20190301.csv\")\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df_edges = pd.read_csv(data_path)\n",
    "df_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the edges loaded in, but need a unique set of nodes for our network. Just in case there are some nodes listed in `source_base_path` but not `sink_base_path`, we need to take a unique set across both columns. \n",
    "\n",
    "This [SO answer](https://stackoverflow.com/a/26977495) will help us get an array of unique nodes. We will then create a pandas DataFrame, and arbitrarily set the index as `node_id`. Note we have sorted the array of unique nodes for ease here so that the main GOV.UK page slug (`/`) has node ID `0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an array of unique nodes\n",
    "unique_nodes = pd.unique(df_edges[[\"source_base_path\", \"sink_base_path\"]].values.ravel(\"K\"))\n",
    "\n",
    "# Sort the nodes alphabetically, and artifically create a 'node_id' column using the index\n",
    "df_nodes = pd.DataFrame(sorted(unique_nodes), columns=[\"nodes\"])\n",
    "df_nodes.index.name = \"node_id\"\n",
    "df_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, let's label the node IDs for both source and sink pages in `df_edges`.\n",
    "\n",
    "Here, we merge `df_nodes` to `df_edges` for both `source_base_path` and `sink_base_path` columns. The chained `reset_index` and `rename` functions on `df_nodes` ensure we don't create duplicate source/sink slug columns during the merge, and also create unique node ID column names for the source/sink slug columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the node IDs back in for each of the sources and sinks\n",
    "for col in df_edges.columns:\n",
    "    if \"_base_path\" in col:\n",
    "        df_edges = df_edges.merge(df_nodes.reset_index().rename(columns={\"nodes\": col, \"node_id\": col+\"_id\"}),\n",
    "                                  on=col,\n",
    "                                  how=\"left\", \n",
    "                                  validate=\"m:1\")\n",
    "df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
