{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOV.UK functional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work through a tutorial to learn about key network metrics and statistics based on this [blog post](https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python#fnref:import). We make a network (nodes and edges csv) using `python src/data/make_network_data.py processed_journey doo_prelim_meta_standard_with_pageseq_from_29-10_to_04-11-2018 processed_network for_networkx_tutorial` command in the terminal with our [data pipeline package](https://github.com/alphagov/govuk-network-data). This is a weeks worth of data where we dropped one off journeys (rare journeys). Data can be found on our the relevant Google drive, [here](https://drive.google.com/drive/u/0/folders/1JDRkoNiNc_gywDM7XabjIWjsaqTZ5md8). \n",
    "\n",
    "This tutorial will take us through the basics of network viz and analysis using NetworkX and Python viz tools. For futher ideas see the [list of algorithms](https://networkx.github.io/documentation/stable/reference/algorithms/index.html) available for use with networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "import community # This is the python-louvain package\n",
    "import re # regular expressions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# http://holoviews.org/user_guide/Network_Graphs.html\n",
    "# pyviz\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from holoviews.element.graphs import layout_nodes\n",
    "from datashader.layout import forceatlas2_layout, random_layout\n",
    "\n",
    "# interactivity for plots\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use global env data dir if desired here\n",
    "input_dir = '../data/processed_network/'\n",
    "nodes_path = os.path.join(input_dir, \"for_networkx_tutorial_nodes.csv.gz\")\n",
    "edges_path = os.path.join(input_dir, \"for_networkx_tutorial_edges.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "Rather than using the method of reading in the data used in the Quaker tutorial, we opt for a pandas approach. This is as the data has some non-ascii characters that were causing some issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes = pd.read_csv(nodes_path, sep='\\t', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = pd.read_csv(edges_path, sep='\\t', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to generate the desired `nodes` list of lists, `node_names` list and `edges` list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a list of arrays (more performant than list of lists)\n",
    "# take the data less the header row\n",
    "# nodes = list(df_nodes.Node.values)\n",
    "nodes = [n for n in list(df_nodes.values)]\n",
    "nodes[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert to list of lists if required, as in this [SO answer](https://stackoverflow.com/questions/45404344/converting-list-of-arrays-to-list-of-lists/45404389). Using arrays might be more performant but we stick to the tutorial approach for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about a simple list comprehension\n",
    "# this doesn't affect node_names, using this or previous has no diff\n",
    "nodes = [list(x) for x in nodes]\n",
    "nodes[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names = [n[0] for n in nodes] # Get a list of only the node names\n",
    "node_names[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns needed, we'll use page url\n",
    "# get the source and destination names as a list of tuples\n",
    "edges = [tuple(e) for e in list(df_edges[['Source_node', 'Destination_node']].values)] # Retrieve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from to, list of tuples as in Quaker example\n",
    "edges[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(node_names))\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have your data as two Python lists: a list of nodes (node_names) and a list of edges (edges). In NetworkX, you can put these two lists together into a single network object that understands how nodes and edges are related. This object is called a Graph, referring to one of the common terms for data organized as a network [n.b. it does not refer to any visual representation of the data. Graph here is used purely in a mathematical, network analysis sense.] First you must initialize a Graph object with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some attribute data to the graph to help us remember what it's about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph(query_used=\"doo_prelim_meta_standard_with_pageseq_from_29-10_to_04-11-2018\")\n",
    "G.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a new Graph object, G, with nothing in it (except 'query_used'). Now you can add your lists of nodes and edges like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(node_names)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick way of getting some general information about your graph, but as you’ll learn in subsequent sections, it is only scratching the surface of what NetworkX can tell you about your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Attributes\n",
    "For NetworkX, a Graph object is one big thing (your network) made up of two kinds of smaller things (your nodes and your edges). So far you’ve uploaded nodes and edges (as pairs of nodes), but NetworkX allows you to add attributes to both nodes and edges, providing more information about each of them. Later on in this tutorial, you’ll be running metrics and adding some of the results back to the Graph as attributes. For now, let’s make sure your Graph contains all of the attributes that are currently in our CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll want to return to a list you created at the beginning of your script: nodes. This list contains all of the rows from quakers_nodelist.csv, including columns for name, historical significance, gender, birth year, death year, and SDFB ID. You’ll want to loop through this list and add this information to our graph. There are a couple ways to do this, but NetworkX provides two convenient functions for adding attributes to all of a Graph’s nodes or edges at once: `nx.set_node_attributes()` and `nx.set_edge_attributes()`. To use these functions, you’ll need your attribute data to be in the form of a Python dictionary, in which node names are the keys and the attributes you want to add are the values. You’ll want to create a dictionary for each one of your attributes, and then add them using the functions above. The first thing you must do is create five empty dictionaries, using curly braces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remind ourselves\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dicitonary is a key-value pair, like a word-definition pair in a traditional dictionary\n",
    "node_id_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop through our nodes list and add the appropriate items to each dictionary. We do this by knowing in advance the position, or index, of each attribute. Because our csv file is well-organized (and from a data pipeline), we know that the node’s name will always be the first item in the list: index 0, since you always start counting with 0 in Python. The nodes’s id will be index 1 (as we add more functionality to the data pipeline this could change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this code uses brackets in two ways. \n",
    "# It uses numbers in brackets to access specific indices within a node list (for example, the birth year at node[4]),\n",
    "# but it also uses brackets to assign a key (always node[0], the ID)\n",
    "# to any one of our empty dictionaries: dictionary[key] = value. Convenient!\n",
    "for node in nodes: # Loop through the list, one row at a time\n",
    "    node_id_dict[node[0]] = node[1]\n",
    "\n",
    "# look up the node_id of the node named 'gov.uk/'\n",
    "node_id_dict['/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a set of dictionaries that you can use to add attributes to nodes in your Graph object. The `set_node_attributes` function takes three variables: the Graph to which you’re adding the attribute, the dictionary of id-attribute pairs, and the name of the new attribute. The code for adding our attributes looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, node_id_dict, 'node_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of your nodes have these attributes, and you can access them at any time. For example, you can print out all the `node_ids` of your nodes by looping through them and accessing the node_id attribute, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n in G.nodes(): # Loop through every node, in our data \"n\" will be the name of the page\n",
    "#    print(n, G.node[n]['node_id']) # Access every node by its name, and then by the attribute \"node_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this statement, you’ll get a line of output for each node in the network. It should look like a simple list of names and id but will be very long:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’ve learned how to create a Graph object and add attributes to it. In the next section, you’ll learn about a variety of metrics available in NetworkX and how to access them. But relax, you’ve now learned the bulk of the code you’ll need for the rest of the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics available in NetworkX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start work on a new dataset, it’s a good idea to get a general sense of the data. The first step, described above, is to simply open the files and see what’s inside. Because it’s a network, you know there will be nodes and edges, but how many of each are there? What information is appended to each node or edge?\n",
    "\n",
    "In our case, there are 123204 edges and 58743 nodes. These edges currently don’t have directions (that is, we're assuming a symmetric relationship between pages; this is wrong, users move from one page to another but might not necessarily go back again) nor do they include additional information (we actually have weights for them which are the occurrences of users moving between these pages). For nodes, we know their names and id but we could also get additional information about the page (taxon, main or whitehall, page metadata etc.).\n",
    "\n",
    "These details inform what you can or should do with your dataset. Too few nodes (say, 15), and a network analysis is less useful than drawing a picture or doing some reading; too many (say, 15 million), and you should consider starting with a subset or finding a supercomputer. Ours appears to be just right, where we can leverage network analysis to make better use of the data. We could also take a subgraph for areas of particularly interest (a GOV.UK project team could make a sub-graph if they have a list of pages they want).\n",
    "\n",
    "The network’s properties also guide your analysis. Because this network is currently undirected, your analysis must use metrics that require symmetric edges between nodes. For example, you can determine what communities pages find themselves in, but you can’t determine the directional routes through which information might flow along the network (you’d need a directed network for that; which we'll look to implement in another notebook). By using the symmetric, undirected relationships in this case, you’ll be able to find sub-communities and the pages who are important to those communities, a process that would be more difficult (though still possible) with a directed network. NetworkX allows you to perform most analyses you might conceive, but you must understand the affordances of your dataset and realize some NetworkX algorithms are more appropriate than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Shape of the Network\n",
    "After seeing what the dataset looks like, it’s important to see what the network looks like. They’re different things. The dataset is an abstract representation of what you assume to be connections between entities; the network is the specific instantiation of those assumptions. The network, at least in this context, is how the computer reads the connections you encoded in a dataset. A network has a topology, or a connective shape, that could be centralized or decentralized; dense or sparse; cyclical or linear. A dataset does not, outside the structure of the table it’s written in.\n",
    "\n",
    "The network’s shape and basic properties will give you a handle on what you’re working with and what analyses seem reasonable. You already know the number of nodes and edges, but what does the network ‘look’ like? Do nodes cluster together, or are they equally spread out? Are there complex structures, or is every node arranged along a straight line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the methods associated with teh graph object by tabbing through\n",
    "G.number_of_selfloops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding = dict(x=(-10, 10), y=(-10, 10))\n",
    "# simple_graph = hv.Graph.from_networkx(G, nx.layout.circular_layout).redim.range(**padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %opts Graph [width=400 height=400]\n",
    "# simple_graph # uncomment to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a sub-graph - Brexit\n",
    "This graph is quite big and a bit slow to work with. We take a subgraph of `G`. If we had a list of pages we could use these directly, as that's how our nodes are indexed. We could also use our dictionary to translate into `node_id` by which to subset. We demonstrate this here using a list of nodes' urls that contain the word \"brexit\" then proceed to do some basic visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes with brexit in the label, or brexit in the page url\n",
    "pages_of_interest = [n for n in node_names if re.findall('brexit', n)]\n",
    "\n",
    "print(\"Here's an example of some page urls with brexit in:\", pages_of_interest[0:2])\n",
    "print(\"The number of nodes in this subgraph is: \", len(pages_of_interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select nodes of interest\n",
    "G_sub = G.subgraph(pages_of_interest)  \n",
    "print(nx.info(G_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%opts Graph [width=400 height=400]\n",
    "\n",
    "simple_graph = hv.Graph.from_networkx(G_sub, nx.circular_layout)\n",
    "simple_graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bundling graphs\n",
    "The datashader library provides algorithms for bundling the edges of a graph and HoloViews provides convenient wrappers around the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the %%opts ipython magic to set some display properties\n",
    "%opts Graph [width=600 height=600 xaxis=None yaxis=None bgcolor=\"white\" tools=[\"hover\"]]\n",
    "### Bundling graphs, takes a while\n",
    "bundled = hd.bundle_graph(simple_graph)\n",
    "bundled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datashading graphs\n",
    "For graphs with a large number of edges we can datashade the paths and display the nodes separately. This loses some of the interactive features but will let you visualize quite large graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd.datashade(bundled, normalization='linear', width=800, height=800) * bundled.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying selections \n",
    "Alternatively we can select the nodes and edges by an attribute that resides on either. In this case we will select the nodes and edges for a particular circle and then overlay just the selected part of the graph on the datashaded plot. Note that selections on the Graph itself will select all nodes that connect to one of the selected nodes. In this way a smaller subgraph can be highlighted and the larger graph can be datashaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be more useful, if it was taxon or mainstream pages for example\n",
    "# this doesnt seem to work atm...\n",
    "bundled.select(node_id_dict=24265, selection_mode='nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prettification of subgraph\n",
    "We can refine further by following various blogs, such as this [one](http://stanmart.github.io/network/economics/visualization/holoviews/datashader/gephi/2018/10/27/econ_network_2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for later, when we've got other node attributes of interest, such as taxon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animating graphs \n",
    "Like all other elements Graph can be updated in a HoloMap or DynamicMap . Here we animate how the Fruchterman-Reingold force-directed algorithm lays out the nodes in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(iteration):\n",
    "    np.random.seed(10)\n",
    "    return hv.Graph.from_networkx(G_sub, nx.spring_layout, iterations=iteration)\n",
    "\n",
    "hv.HoloMap({i: get_graph(i) for i in range(5, 30, 5)},\n",
    "           kdims='Iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks pretty, but only gets you so far..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of ways to visualize a network, and a force-directed layout, of which the above image is an example, is among the most common. Force-directed graphs attempt to find the optimum placement for nodes with a calculation based on the tension of springs in Hooke’s Law, which for smaller graphs often creates clean, easy-to-read visualizations. The visualization embedded above shows you there is a single large component of connected nodes (in the center) and several small components with just one or two connections around the edges (this is to be expected as our page list is mainly for demo purposes; there's some user searches for brexit that are picked up). This is a fairly common network structure. Knowing that there are multiple components in the network will usefully limit the calculations you’ll want to perform on it. By displaying the number of connections (known as degree, see below) as the size of nodes, the visualization also shows that there are a few nodes with lots of connections that keep the central component tied together (although there's not any obvious hubs here). These large nodes are known as hubs, and the fact that they show up so clearly here gives you a clue as to what you’ll find when you measure centrality in the next section.\n",
    "\n",
    "Visualizations, however, only get you so far. The more networks you work with, the more you realize most appear similar enough that it’s hard to tell one from the next. Quantitative metrics let you differentiate networks, learn about their topologies, and turn a jumble of nodes and edges into something you can learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the GOV.UK graph (size is only an issue for viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good metric to begin with is network density. This is simply the ratio of actual edges in the network to all possible edges in the network. In an undirected network like this one, there could be a single edge between any two nodes, but as you saw in the visualization, only a few of those possible edges are actually present. Network density gives you a quick sense of how closely knit your network is.\n",
    "\n",
    "And the good news is many of these metrics require simple, one-line commands in Python. From here forward, you can keep building on your code block from the previous sections. You don’t have to delete anything you’ve already typed, and because you created your network object `G` in the codeblock above, all the metrics from here on should work correctly.\n",
    "\n",
    "You can calculate network density by running `nx.density(G)`. However the best way to do this is to store your metric in a variable for future reference, and print that variable, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = nx.density(G)\n",
    "round(density, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = nx.density(G_sub)\n",
    "round(density, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of density is a number, so that’s what you’ll see when you print the value. In this case, the density of our entire functional network is approximately 0.00007. On a scale of 0 to 1, not a very dense network. This contrasts with our \"Brexit\" subgraph which wasn't that dense but relatively 100 times moreso than the rest of GOV.UK. A 0 would mean that there are no connections at all, and a 1 would indicate that all possible edges are present (a perfectly connected network): this GOV.UK network is on the lower end of that scale, and close to 0. It would be interesting if we had a list of mainstream pages and compared density to the subgraph of Whitehall or specialist pages. Perhaps \n",
    "\n",
    "A shortest path measurement is a bit more complex. It calculates the shortest possible series of nodes and edges that stand between any two nodes, something hard to see in large network visualizations. This measure is essentially finding friends-of-friends—if my mother knows someone that I don’t, then mom is the shortest path between me and that page. The Six Degrees of Kevin Bacon game, from which our project takes its name, is basically a game of finding shortest paths (with a path length of six or less) from Kevin Bacon to any other actor.\n",
    "\n",
    "To calculate a shortest path, you’ll need to pass several input variables (information you give to a Python function): the whole graph, your source node, and your target node. Let’s find the shortest path between two Brexit related pages. Since we used names to uniquely identify our nodes in the network, you can access those nodes (as the source and target of your path), using the names directly. Or if the names are very long to type out, we might prefer to use node id (from this [SO answer](https://stackoverflow.com/questions/8023306/get-key-by-value-in-dictionary) we get a key by a value in the dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have some pages in mind, find the shortest functional path\n",
    "# i got their value or node id by looking at the viz\n",
    "pg1 = list(node_id_dict.keys())[list(node_id_dict.values()).index(8445)]\n",
    "pg2 = list(node_id_dict.keys())[list(node_id_dict.values()).index(8346)]\n",
    "\n",
    "print(pg1, '\\n', pg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_path_of_interest = nx.shortest_path(G_sub, source=pg1, target=pg2)\n",
    "\n",
    "print(\"Shortest path between /government/publications/trade-marks-and-designs-if-theres-no-brexit-deal and /government/publications/trading-with-the-eu-if-theres-no-brexit-deal:\",\n",
    "      a_path_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the size of your network, this could take a little while to calculate, since Python first finds all possible paths and then picks the shortest one. The output of shortest_path will be a list of the nodes that includes the “source”, the “target”, and the nodes between them. This isn't that informative here, but we might imagine if lots of shortest paths run through a particular page it could be considered a hub.\n",
    "\n",
    "Python includes many tools that calculate shortest paths. There are functions for the lengths of shortest paths, for all shortest paths, and for whether or not a path exists at all in the documentation. You could use a separate function to find out the length of the Fell-Whitehead path we just calculated, or you could simply take the length of the list minus one (We take the length of the list minus one because we want the number of edges (or steps) between the nodes listed here, rather than the number of nodes.), like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of that path:\", len(a_path_of_interest)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many network metrics derived from shortest path lengths. One such measure is diameter, which is the longest of all shortest paths. After calculating all shortest paths between every possible pair of nodes in the network, diameter is the length of the path between the two nodes that are furthest apart. The measure is designed to give you a sense of the network’s overall size, the distance from one end of the network to another.\n",
    "\n",
    "Diameter uses a simple command: `nx.diameter(G)`. However, running this command on the Quaker graph will yield an error telling you the Graph is “not connected.” This simply means that your graph, as you already saw, has more than one component. Because there are some nodes that have no path at all to others, it is impossible to find all of the shortest paths. Take another look at the visualization of your graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.diameter(G_sub) # errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no shortest path between nodes of one component and nodes of another, nx.diameter() returns the “not connected” error. You can remedy this by first finding out if your Graph “is connected” (i.e. all one component) and, if not connected, finding the largest component and calculating diameter on that component alone. Here’s the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your Graph has more than one component, this will return False:\n",
    "print(nx.is_connected(G_sub))\n",
    "\n",
    "# Next, use nx.connected_components to get the list of components,\n",
    "# then use the max() command to find the largest one:\n",
    "components = nx.connected_components(G_sub)\n",
    "largest_component = max(components, key=len)\n",
    "\n",
    "# Create a \"subgraph\" of just the largest component\n",
    "# Then calculate the diameter of the subgraph, just like you did with density.\n",
    "#\n",
    "\n",
    "subgraph = G_sub.subgraph(largest_component)\n",
    "diameter = nx.diameter(subgraph)\n",
    "print(\"Network diameter of largest component:\", diameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we took the largest component, we can assume there is no larger diameter for the other components. Therefore this figure is a good stand in for the diameter of the whole Graph. The network diameter of this network’s largest component is 20: there is a path length of 20 between the two farthest-apart nodes in the network. Unlike density which is scaled from 0 to 1, it is difficult to know from this number alone whether 20 is a large or small diameter. For some global metrics, it can be best to compare it to networks of similar size and shape (The most principled way of doing this kind of comparison is to create random graphs of identical size to see if the metrics differ from the norm. NetworkX offers plenty of tools for generating random graphs.).\n",
    "\n",
    "The final structural calculation you will make on this network concerns the concept of triadic closure. Triadic closure supposes that if two pages know the same page, they are likely to know each other. If C knows both A and B, then A and B may very well know each other, completing a triangle in the visualization of three edges connecting C, B, and C. The number of these enclosed triangles in the network can be used to find clusters and communities of individuals that all know each other fairly well.\n",
    "\n",
    "One way of measuring triadic closure is called clustering coefficient because of this clustering tendency, but the structural network measure you will learn is known as transitivity (Why is it called transitivity? You might remember the transitive property from high school geometry: if A=B and B=C, the A must equal C. Similarly, in triadic closure, if person A knows page B and page B knows page C, then page A probably knows page C: hence, transitivity.). Transitivity is the ratio of all triangles over all possible triangles. A possible triangle exists when one page (C) knows two pages (A and B). So transitivity, like density, expresses how interconnected a graph is in terms of a ratio of actual over possible connections. Remember, measurements like transitivity and density concern likelihoods rather than certainties. All the outputs of your Python script must be interpreted, like any other object of research. Transitivity allows you a way of thinking about all the relationships in your graph that may exist but currently do not.\n",
    "\n",
    "You can calculate transitivity in one line, the same way you calculated density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triadic_closure = nx.transitivity(G_sub)\n",
    "print(\"Triadic closure:\", triadic_closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also like density, transitivity is scaled from 0 to 1, and you can see that the network’s transitivity is about 0.18, much higher than its 0.0092 density. Because the graph is not very dense, there are fewer possible triangles to begin with, which may result in slightly higher transitivity. That is, nodes that already have lots of connections are likely to be part of these enclosed triangles. To back this up, you’ll want to know more about nodes with many connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality\n",
    "After getting some basic measures of the entire network structure, a good next step is to find which nodes are the most important ones in your network. In network analysis, measures of the importance of nodes are referred to as centrality measures. Because there are many ways of approaching the question “Which nodes are the most important?” there are many different ways of calculating centrality. Here you’ll learn about three of the most common centrality measures: degree, betweenness centrality, and eigenvector centrality.\n",
    "\n",
    "Degree is the simplest and the most common way of finding important nodes. A node’s degree is the sum of its edges. If a node has three lines extending from it to other nodes, its degree is three. Five edges, its degree is five. It’s really that simple. Since each of those edges will always have a node on the other end, you might think of degree as the number of pages to which a given page is directly connected. The nodes with the highest degree in a social network are the pages who know the most pages. These nodes are often referred to as hubs, and calculating degree is the quickest way of identifying hubs.\n",
    "\n",
    "Calculating centrality for each node in NetworkX is not quite as simple as the network-wide metrics above, but it still involves one-line commands. All of the centrality commands you’ll learn in this section produce dictionaries in which the keys are nodes and the values are centrality measures. That means they’re ready-made to add back into your network as a node attribute, like you did in the last section. Start by calculating degree and adding it as an attribute to your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do it for the whole graph here\n",
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just ran the `G.degree()` method on the full list of nodes in your network (`G.nodes()`). Since you added it as an attribute, you can now see `gov.uk/` ’s degree along with its other information if you access his node directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.node['/'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But these results are useful for more than just adding attributes to your Graph object. Since you’re already in Python, you can sort and compare them. You can use the built-in function `sorted()` to sort a dictionary by its keys or values and find the top twenty nodes ranked by degree. To do this you’ll need to use itemgetter, which we imported back at the beginning of the tutorial. Using sorted and `itemgetter`, you can sort the dictionary of degrees like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `gov.uk/`’s degree is 4766, relatively high for this network. But printing out this ranking information illustrates the limitations of degree as a centrality measure. You probably didn’t need NetworkX to tell you that the home page, was important. Most social networks will have just a few hubs of very high degree, with the rest of similar, much lower degree. Degree can tell you about the biggest hubs, but it can’t tell you that much about the rest of the nodes. And in many cases, those hubs it’s telling you about are not especially surprising. In this case almost all of the hubs are mainstream content.\n",
    "\n",
    "Thankfully there are other centrality measures that can tell you about more than just hubs. Eigenvector centrality is a kind of extension of degree—it looks at a combination of a node’s edges and the edges of that node’s neighbors. Eigenvector centrality cares if you are a hub, but it also cares how many hubs you are connected to. It’s calculated as a value from 0 to 1: the closer to one, the greater the centrality. Eigenvector centrality is useful for understanding which nodes can get information to many other nodes quickly. If you know a lot of well-connected pages, you could spread a message very efficiently. If you’ve used Google, then you’re already somewhat familiar with Eigenvector centrality. Their PageRank algorithm uses an extension of this formula to decide which webpages get to the top of its search results. We can use this metric on our subgraph to identify pages with lots of well-connected neighbours.\n",
    "\n",
    "Betweenness centrality is a bit different from the other two measures in that it doesn’t care about the number of edges any one node or set of nodes has. Betweenness centrality looks at all the shortest paths that pass through a particular node (see above). To do this, it must first calculate every possible shortest path in your network, so keep in mind that betweenness centrality will take longer to calculate than other centrality measures (but it won’t be an issue in a dataset of this size). Betweenness centrality, which is also expressed on a scale of 0 to 1, is fairly good at finding nodes that connect two otherwise disparate parts of a network. If you’re the only thing connecting two clusters, every communication between those clusters has to pass through you. In contrast to a hub, this sort of node is often referred to as a broker. Betweenness centrality is not the only way of finding brokerage (and other methods are more systematic), but it’s a quick way of giving you a sense of which nodes are important not because they have lots of connections themselves but because they stand between groups, giving the network connectivity and cohesion.\n",
    "\n",
    "These two centrality measures are even simpler to run than degree—they don’t need to be fed a list of nodes, just the graph `G`. You can run them with these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G_sub.degree(G_sub.nodes())) # Rerun for Brexit subgraph\n",
    "betweenness_dict = nx.betweenness_centrality(G_sub) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G_sub) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(G_sub, degree_dict, 'degree')\n",
    "nx.set_node_attributes(G_sub, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G_sub, eigenvector_dict, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check they've been added\n",
    "G_sub.node['/government/brexit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 10 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:10]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eigenvector = sorted(eigenvector_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 10 nodes by eigenvector-ness:\")\n",
    "for b in sorted_eigenvector[:10]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll notice that many, but not all, of the nodes that have high degree also have high betweenness centrality. In fact, betweeness centrality might surface some useful insights, especially if we include additional metadata as attributes for our nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First get the top 10 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:10]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm from these results that some pages, like `/government/publications/structuring-your-business-if-theres-no-brexit-deal--2`, has high betweenness centrality but low degree. This could mean that these pages are important brokers, connecting otherwise disparate parts of the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced NetworkX: Community detection with modularity\n",
    "Another common thing to ask about a network dataset is what the subgroups or communities are within the larger social structure. Is your network one big, happy family where everyone knows everyone else? Or is it a collection of smaller subgroups that are only connected by one or two intermediaries? The field of community detection in networks is designed to answer these questions. There are many ways of calculating communities, cliques, and clusters in your network, but the most popular method currently is modularity. Modularity is a measure of relative density in your network: a community (called a module or modularity class) has high density relative to other nodes within its module but low density with those outside. Modularity gives you an overall score of how fractious your network is, and that score can be used to partition the network and return the individual communities.13\n",
    "\n",
    "Very dense networks are often more difficult to split into sensible partitions. Luckily, as you discovered earlier, this network is not all that dense. There aren’t nearly as many actual connections as possible connections, and there are several altogether disconnected components. Its worthwhile partitioning this sparse network with modularity and seeing if the result make historical and analytical sense.\n",
    "\n",
    "Community detection and partitioning in NetworkX requires a little more setup than some of the other metrics. There are some built-in approaches to community detection (like minimum cut, but modularity is not included with NetworkX. Fortunately there’s an additional python module you can use with NetworkX, which you already installed and imported at the beginning of this tutorial. You can read the full documentation for all of the functions it offers, but for most community detection purposes you’ll only want best_partition():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = community.best_partition(G_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will create a dictionary just like the ones created by centrality functions. `best_partition()` tries to determine the number of communities appropriate for the graph, and assigns each node a number (starting at 0), corresponding to the community it’s a member of. You can add these values to your network in the now-familiar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G_sub, communities, 'modularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get a list of just the nodes in that class\n",
    "class0 = [n for n in G_sub.nodes() if G_sub.node[n]['modularity'] == 0]\n",
    "\n",
    "# Then create a dictionary of the eigenvector centralities of those nodes\n",
    "class0_eigenvector = {n:G_sub.node[n]['eigenvector'] for n in class0}\n",
    "\n",
    "# Then sort that dictionary and print the first 5 results\n",
    "class0_sorted_by_eigenvector = sorted(class0_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Modularity Class 0 Sorted by Eigenvector Centrality:\")\n",
    "for node in class0_sorted_by_eigenvector[:5]:\n",
    "    print(\"Name:\", node[0], \"| Eigenvector Centrality:\", node[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using eigenvector centrality as a ranking can give you a sense of the important pages within this modularity class. You’ll need some expert knowledge of the data to probably glean insights here, so it's worth talking to the programme team as to why these pages are clustered. With just a little bit of digging, we can get an indication that modularity is probably working as expected.\n",
    "\n",
    "In smaller networks like this one, a common task is to find and list all of the modularity classes and their members. You can do this by manipulating the communities dictionary. You’ll need to reverse the keys and values of this dictionary so that the keys are the modularity class numbers and the values are lists of names. You can do so like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity = {} # Create a new, empty dictionary\n",
    "for k,v in communities.items(): # Loop through the community dictionary\n",
    "    if v not in modularity:\n",
    "        modularity[v] = [k] # Add a new key for a modularity class the code hasn't seen before\n",
    "    else:\n",
    "        modularity[v].append(k) # Append a name to the list for a modularity class the code has already seen\n",
    "\n",
    "for k,v in modularity.items(): # Loop through the new dictionary\n",
    "    if len(v) > 2: # Filter out modularity classes with 2 or fewer nodes\n",
    "        print('Class '+str(k)+':', v) # Print out the classes and their members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the code above that you are filtering out any modularity classes with two or fewer nodes, in the line if `len(v) > 2`. You’ll remember from the visualization that there were lots of small components of the network with only two nodes. Modularity will find these components and treat them as separate classes (since they’re not connected to anything else). By filtering them out, you get a better sense of the larger modularity classes within the network’s main component. We should adjust this threshold accordingly, based on the specific problem and context of your question.\n",
    "\n",
    "Working with NetworkX alone will get you far, and you can find out a lot about modularity classes just by working with the data directly. But you’ll almost always want to visualize your data (and perhaps to express modularity as node color). In the next section you’ll learn how to export your NetworkX data for use in other programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G_sub.node['/government/brexit'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data\n",
    "NetworkX supports a very large number of file formats for data export. If you wanted to export a plaintext edgelist to load into Palladio, there’s a convenient wrapper for that. Frequently at Six Degrees of Francis Bacon, we export NetworkX data in D3’s specialized JSON format, for visualization in the browser. You could even export your graph as a Pandas dataframe if there were more advanced statistical operations you wanted to run. There are lots of options, and if you’ve been diligently adding all your metrics back into your Graph object as attributes, all your data will be exported in one fell swoop.\n",
    "\n",
    "Most of the export options work in roughly the same way, so for this tutorial you’ll learn how to export your data into Gephi’s GEXF format. Once you’ve exported the file, you can upload it directly into Gephi for visualization.\n",
    "\n",
    "Exporting data is often a simple one-line command. All you have to choose is a filename. In this case we’ll use quaker_network.gexf. To export type:\n",
    "\n",
    "That’s it! When you run your Python script, it will automatically place the new GEXF file in the same directory as your Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our brexit subgraph (remember this is not all brexit content, jsut brexit in the url)\n",
    "nx.info(G_sub)\n",
    "# nx.write_gexf(G, 'quaker_network.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial we read in our node and edge csv.gz from our data pipeline. We created a graph object with the networkx and then calculated a variety of metrics that could be added as attributes of the nodes. Likewise we could use a similar approach for the edges if so inclined. We also used some pyviz modules to visualise the topology of a network. The could developed could be used to extend our data pipeline, so that users can create their own graphs of relevant pages (by inputing a page list as an arguemnt). This could run and compute the node attributes, outputting a graph for downstream applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
