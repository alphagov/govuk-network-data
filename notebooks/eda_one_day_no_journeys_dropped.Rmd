---
title: "eda_user_journeys_no_drops"
author: "Matthew Gregory"
date: "06/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## EDA no journeys dropped

Data from **one day**, the 31 Oct 2018.  

Using `prelim_meta_standard_query_with_pageseq` sql query.  

During the `make_dataset` step we did not drop any page sequences (kept the rare ones, one occurrence in a day; also kept journeys of just one page).  

## Computing environment

Check the session info at the end or you could run in Docker. Try the following command to ensure a computing environment that will do the job. This will open a RStudio instance on your browser. Go to your local host 8787. You can log in by typing "rstudio" for username and by typing in the password that you specify when you initiate the container, as below.     

```{r eval = FALSE}
docker run -e PASSWORD=i<3hadley -p 8787:8787 rocker/tidyverse:3.5.0
```


## Data pipeline version

Queried and preprocessed using commit 2f83eef.  

## Read in the data

If you're using rocker/tidyverse then upload the data into your working directory, using the button in the GUI is straightforward. You can then proceed. If doing it locally, we assume the data is in the working directory you are working from.  

```{r message=FALSE}
library(tidyverse)
library(ggthemes)

```

```{r}
# might take a while, you should get progress bar after a min
# could use fread from data.table if you need faster option
df <- read_csv("processed_2018-10-31-prelim_meta_standard_query_with_pageseq_no_dropping.csv.gz",
                           col_names = TRUE)
```

Peek at the data.

```{r}
head(df)
```

Ah, of course, it's output for python and pandas. We'd need a column tibble for the dictionaries, like `DeviceCategories`. We should stay in python if interested in those variables. However, we can explore some of the other aspects, such as what typical journeys and ignore the meta data for now.  

```{r}
tail(df)
```

Glimpse the variables types. Data types look correct.  

```{r}
glimpse(df)
```

For the documentation on these variables see the [README](https://github.com/ukgovdatascience/govuk-network-data) for our data pipeline package.  

## Missing data

```{r}
extracat::visna(df, sort = "b")

```

A small number of rows are missing `PageSequence` and `Page_List`. This suggests that there is a problem in the data pipeline in getting `PageSequence` for these rows. Let's inspect the sequence to see if we can spot anything odd. Of course the issue could be having been read into R, and not with the pipeline.  

It also could be things inherently wonky, such as a Sequence that is all events, or is recorded as such. It could be things like banner shown, click a link that fires an event (not page hit) and then exit. Also could be due to set up, as some page hits are captured as events internally.  

```{r}
sum(!complete.cases(df)) / nrow(df)
```

Less than one percent are missing.

```{r}
# filter for missingness
dodgy <- df[!complete.cases(df), ]

```

```{r}
select(dodgy, Sequence, PageSequence, Page_Seq_Occurrences)
```

```{r}
# free up some space
rm(dodgy)
```

Let's drop rows with missing `PageSequence` data. They look to be Sequences without Page hits, jsut events.  

```{r}
df <- df %>%
  drop_na()

# should equal zero
# sum(!complete.cases(df))
```


## A weekday's data

```{r}
dim(df)
```

The data gets quite big when we don't drop unique journeys, hence we only use one day, a Wednesday. Might be better to this for a few days then sample, however at the moment our pipeline might struggle (i.e. if running on our local machines unless we pay for extra compute).  
## What's a typical journey?

## The data story

Big query provides us with a generated `Sequence` of events & page hits. We use our data pipeline package we built to process the data into a useful form for typical GOV.UK project team needs.   

```{r}
head(df$Sequence, 2)

```

Given this captures event information, behaviour of the user in how they are interacting with the page (in a pre-defined way), understandably you might expect these journeys to be nuanced and more likely to be unique (i.e. I went to this page then download link clicked here, then back, then clicked there, then clicked on the survey at the footer).  

Above we see two journeys, the first where a specific page is hit, then the journey (ergo the session) ends. The second user journey shows a more nuanced journey including behaviour of the user on the pages they visit (events).  

FYI: (The `PAGE<:<NULL<:<NULL` event means a page hit).  

We drop the events and seperate the different pages with `>>` to create the `PageSequence` that was travelled at least once by a user in one session (during this particular day, given our data; this is directly queried from BigQuery rather than transforming `Sequence`). As this is just on one day, we might be biasing our interpretation, as we will likely see more unique journeys, than if used data over a longer time period.    

```{r}
head(df$Page_Seq_Occurrences, 2)

```

This seems to be more in-line with what colleagues around GOV.UK would consider a user journey. A sequences of pages that they visit. They would consider someone visiting A >> B, equivalent to someone who goes A >> downloads something >> B. Although this is debatable, we assume this is the case at least at the macro level, where we are trying to work out what type of journeys users are taking (are they simple one or two pages, or are they nuanced and complicated with loops and backtracking? etc.).  

`Occurrences` counts the number of times (sessions) the Sequence was identified. Given the above paragraph, we are arguably more interested in `Page_Seq_Occurrences` which gives the number of times (sessions) the PageSequence was identified.  

We should caveat this with As the data I’m looking at is for one day only (as I’m not dropping one-offs), we might be biasing our estimate of how common unique journeys are. With more sampling effort (using more days data) a unqiue page sequence might appear again, rendering it no longer a “one-off”.

Reminds me of a [species accumulation curve](https://www.researchgate.net/figure/Species-accumulation-curve-based-on-sampling-effort-outlined-in-Table1_fig2_230591046). Where the x axis could be the number of days sampled and the y the number of unique PageSequences. This is why we typically use a 3-4 day roll-up period in our `make_dataset.py`.  

## What the distribution of the occurences of user journeys by page sequence length?

So within a given day, most journeys are happening just once, they are unique. As the experimental unit is at the `Sequence` level, we should be weary of just counting these. Instead we should look at how many unique Page Sequences there are.  

```{r}
length(unique(df$PageSequence))

```

## Group the data so that `Page_Sequence` is the experimental unit

We need to aggregate the number of `Page_Seq_Occurrences` for each unique `Page_Sequence`. This will give us an indication of how frequently these journeys are happening in a day. Then we can see what the typical journey is, by investigating these `Page_Sequences`.  

```{r}
# make it smaller, more manageable for testing
# replace with all data, after test
set.seed(1337)
df_small <- sample_frac(df, size = 0.3)

```

The data can be considered already melted, we can then easily compute margins, ignoring the `Sequence` nuance. 

Should we sum using `Occurrences` or `Page_Seq_Occurrences`?  
```{r}
filter(df, PageSequence == "/check-mot-history") %>%
  select(PageSequence, Occurrences, Page_Seq_Occurrences)
```

The sum has already been done for us it seems, so `Page_Seq_Occurrences` captures how many sessions involved that `PageSequence` in the timeframe. However, as we've dropped some rows for missingness, let's recalculate this.  

```{r}
pageseq_sum <- df %>%
  group_by(PageSequence) %>%
  # integer overflow, if not as.numeric first
  summarise( journey_occurrences = sum(as.numeric(Occurrences)))

pageseq_sum
```

As you can see, there will be plenty of unique journeys that are junk typed in after the "www.gov.uk". Caveating that, even the weird ones can occur more than once, for example:  

```{r}
pageseq_sum[1337, ]$PageSequence
```

## Frequency distribution of unique user journey PageSequence

```{r}
pageseq_sum %>%
arrange(desc(journey_occurrences))
```

We sense checked these numbers against GA data, at least the ordering of popular pages by page views (page views were approx double the session number which is plausible).  

Or getting just the strings.  

```{r}
pageseq_sum %>%
arrange(desc(journey_occurrences)) %>%
  # override tibble printing 
  #.$PageSequence %>%
  head(10)
```
As a plot and proportions of total occurences of PageSequences. We save as object p so we can add a figure legend.    

```{r}
pageseq_sum %>%
arrange(desc(journey_occurrences)) %>%
  # get the top 10
  head(10) %>%
  # as a prop of total journey occurrences
  mutate(., journey_occurrences_prop = journey_occurrences / sum(pageseq_sum$journey_occurrences)) %>%
  # order by magnitude
  ggplot(., aes(x = reorder(PageSequence, journey_occurrences_prop),
                y = journey_occurrences_prop*100)) +
  geom_bar(stat="identity") +
  # make label readable
  scale_x_discrete(labels = function(x)
    stringr::str_wrap(x, width = 40)) +
  ylab("% of occurrences") + xlab("PageSequence") +
  coord_flip() +
  theme_tufte() +
  # change label font size
    theme(axis.text=element_text(size=8)) -> p
```

Combined these top ten make up the prop of the total PageSeqeunce occurences to giv eour legend info.

```{r}
n <- 10

top_n <- pageseq_sum %>%
arrange(desc(journey_occurrences)) %>%
  # get the top n
  head(n) %>%
  # as a prop of total journey occurrences
  mutate(., journey_occurrences_prop = journey_occurrences / sum(pageseq_sum$journey_occurrences)) 

other <- pageseq_sum %>%
arrange(desc(journey_occurrences)) %>%
  # get all but the top 10
  tail(-n) %>%
  # as a prop of total journey occurrences
  mutate(., journey_occurrences_prop = journey_occurrences / sum(pageseq_sum$journey_occurrences))

```

```{r}
# compare top 10, prop with everyother PageSequence
leg <- paste ("The top", n, "make up",
       round(sum(top_n$journey_occurrences_prop*100), 0),
       "% of the total PageSequence Occurrences.",
       "\n",
       "The other", nrow(pageseq_sum) - n, " PageSequence journeys",
       " make up the remaing",
       round(sum(other$journey_occurrences_prop*100), 0),
"%") 

```
Finally the plot.  

```{r}

p + 
  ggtitle(leg) +
  theme(plot.title = element_text(size=6))
```

### Is there such a thing as a common journey?

These PageSequences only make up 17% of journeys. Not that impressive, let's look at the top 100, albeit losing the labels, and compare to the "other" journeys.  

```{r}

top_journier <- function(n = 10) {
  # needs pageseq_sum to get total occurrences, for prop
  # calc props for top n
  # for the tail pages, aggregate as "all_other_journeys"
  # plot without labels

  
  # top_n is a dplyr funciton
  the_top_n <- pageseq_sum %>%
arrange(desc(journey_occurrences)) %>%
  # get the top n
  head(n) %>%
  # as a prop of total journey occurrences
  mutate(., journey_occurrences_prop = journey_occurrences / sum(pageseq_sum$journey_occurrences)) 

other <- pageseq_sum %>%
arrange(desc(journey_occurrences)) %>%
  # get all but the top 10
  tail(-n) %>%
  # as a prop of total journey occurrences
  mutate(., journey_occurrences_prop = journey_occurrences / sum(pageseq_sum$journey_occurrences))

# create aggregate other category, for those not in top_n
# or could do 1 - top_n, but this is good sense check

other_agg <- sum(other$journey_occurrences_prop)

other_row <- tibble(PageSequence = "Others",
                       journey_occurrences_prop = other_agg)

# rbind other_row to top_n, for input into ggplot
bind_rows(the_top_n, other_row) %>%
  mutate(the_tail = if_else(
    # sum(df$PageSequence == "Others") = 0, safe label
    PageSequence == "Others",
    true = "All others PageSequenes combined",
    false = paste("In the top", n, "PageSequences")),
      # make percentages rather than prop
    journey_occurrences_prop = (journey_occurrences_prop*100)
    ) %>%
  # dataframe for plotting
ggplot(., aes(x = reorder(PageSequence, journey_occurrences_prop),
                y = journey_occurrences_prop, fill = the_tail)) +
  geom_bar(stat="identity") +
  # make pretty
  coord_flip() +
  theme_tufte() +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        # remove legend as is nonsense name
        legend.title = element_blank(),
        # change label size
        axis.title=element_text(size = 8)) +
  ylab("Journey Occurences as percentage of total")-> p

return(p)
}

```

Let's try it out for top 100 journeys.

```{r}
top_journier(100)
```

```{r}
top_journier(10)

```

However, we should be careful, lots of the tail contains these weird junk one-pager journeys. Can we drop the junk and then repeat the above analysis?

```{r}
pageseq_sum %>%
arrange(journey_occurrences)
```

```{r}
hist(pageseq_sum$journey_occurrences)
```

Or as a bin-frequency table: 

```{r}
br <- seq(0, 165000, by = 1000)
total_occurrences <- sum(pageseq_sum$journey_occurrences)

ranges <- paste(head(br,-1), br[-1], sep=" - ")
# hist is efficient at binning
freq <- hist(pageseq_sum$journey_occurrences,
              breaks = br, include.lowest = TRUE,
              plot = FALSE)

# for printing
x_bin <- data_frame(range = ranges, frequency = freq$counts) %>%
  mutate(prop = frequency / sum(frequency))

head(x_bin)

tail(x_bin)

```

Let's plot this.

```{r}
plot(x_bin$prop, type = "l")

# tidy up
rm(br, ranges, freq, x_bin)
```


Most journeys are rare, although let's zoom in a bit to clarify.  

```{r}
ggplot(pageseq_sum, aes(x = journey_occurrences)) +
  geom_histogram(bins = 30) + scale_x_log10() +
  ggthemes::theme_tufte() +
  xlab("Journey Occurrences") + ylab("Count")

```

It seems the most common types of journeys occur fewer than ten times. This can be considered as, if you were to pick a random user session, the chances are it would be a "one-off" `PageSequence`. Looking at these `PageSequences` that are rare (< 10).  

```{r}
pageseq_sum %>%
  filter(journey_occurrences <= 10) %>%
  ggplot(aes(x=journey_occurrences)) + 
  geom_density(stat = "density", adjust = 2) +
  scale_y_continuous(breaks=c(1,3,7,10)) +
  ggthemes::theme_tufte() +
  xlab("Journey Occurrences") + ylab("Density")
```

## Investigate these one-offs

What proportion of our PageSequence journeys are one-offs?

```{r}
one_offs <- pageseq_sum %>%
  filter(journey_occurrences == 1) %>%
  nrow()

# total rows minues one offs
not_one_offs <- nrow(pageseq_sum) - one_offs

# % one_offs
round((one_offs/nrow(pageseq_sum))*100, 0)

# % not
round((not_one_offs/nrow(pageseq_sum))*100, 0)


df_one_offs <- tibble(
  ps_type = c("one-off", "not-one-off"),
  percentage = c(round((one_offs/nrow(pageseq_sum))*100, 0),
                 round((not_one_offs/nrow(pageseq_sum))*100, 0))
)
```

```{r}
ggplot(df_one_offs,
       aes(x = ps_type, y = percentage)) +
  geom_bar(stat = "identity") +
  ggthemes::theme_tufte() +
  xlab("PageSequence occurence type") + ylab("Percentage")
```


## Of the rare journeys, what proportion are one-offs?

If we define a rare journey as one that occurrs fewer than ten times in a day.


```{r}
pageseq_sum %>%
  # inspect the "Rare" less than ten journies
  filter(journey_occurrences <= 10) %>%
with(., table(journey_occurrences)) %>%
  prop.table()
```

# Trying to drop junk

Let's remove any PageSequences that start with "/ ". This space suggests they've typed straight into the url bar, and if it's only occured once then it's probably a typo.  

```{r}
#R needs to backslash the backslash, hence double
junk <- pageseq_sum %>%
  filter(str_detect(PageSequence, "^\\/\\s"))

nrow(junk)
junk
```

There's not acutally that many of these, it's jsut that alphabetical order made me think they were more common! Only 46. We should probably drop them though.  

```{r}
no_junk <- pageseq_sum %>%
  filter(!str_detect(PageSequence, "^\\/\\s"))
```

There's still alot of one-offs. How can we inspect them to make sense of them?

```{r}
no_junk
```

Do pages start with hyphens? Probably not right?

```{r}
# hyphen not special
no_junk <- no_junk %>%
  filter(!str_detect(PageSequence, "^\\/-"))

no_junk
```
More junk, commas and colons can probaly be removed? Not sure about question marks...

```{r}
no_junk <- 
  no_junk %>%
  # drop colons
  filter(!str_detect(PageSequence, "^\\/\\:")) %>%
  # drop 1k ? markers, seem to be when event is clicked on main page
  filter(!str_detect(PageSequence, "^\\/\\?"))
  

no_junk
```

Let's be bold and assume it starts with a character or digit, anything else is junk.  

```{r}
no_junk <- 
  no_junk %>%
  filter(str_detect(PageSequence, "^\\/[a-z A-Z 0-9]"))

no_junk


```

Looks to be some legit that start with numbers.

```{r}
no_junk_one_offs <- no_junk %>%
  filter(journey_occurrences == 1)

# View(no_junk_one_offs)
```

Inspecting the data with the viewer

Scanning through these one-offs they appear genuine
One-offs tend to be either niche, long or journeys that involve search.  


Even after removing the junk, the majority of journeys are one-offs.

```{r}
  one_offs <- no_junk_one_offs %>%
  nrow()

# total rows minues one offs
not_one_offs <- nrow(no_junk) - one_offs


df_one_offs_no_junk <- tibble(
  ps_type = c("one-off", "not-one-off"),
  percentage = c(round((one_offs/nrow(no_junk))*100, 0),
                 round((not_one_offs/nrow(no_junk))*100, 0))
)

ggplot(df_one_offs_no_junk,
       aes(x = ps_type, y = percentage)) +
  geom_bar(stat = "identity") +
  ggthemes::theme_tufte() +
  xlab("PageSequence occurence type") + ylab("Percentage") +
  ggtitle("Junk removed", subtitle = "'one-off' journeys still dominate")
```

So the one-offs have lost a percentage point after removal of the junk. So junk was contributing the one-offs but there wasn't very much of it relatively. It's has negligible impact on our understanding of the most common journeys.  

A common journey type is a misconception. Sure there are some common journeys that draw our attention, but actually most journeys are one-offs, rare or unique over a day.  

As a reminder, here's a selection of some of those journeys.  

```{r}
set.seed(255)

sample_n(no_junk_one_offs, 20) %>%
  select(PageSequence) 
```

GOV.UK tends to focus on what we view the main bits of the site that interact with most users. Although these pages do have a lot of page views and other GA metrics, this work suggests that they are the less typical kind of user. About 80% of users in a day have an unshared experiene of GOV.UK, they make a unique or one-off journey.  

This suggests our users experience is more nuanced than we may be giving credit for.

For example, the journey "/personal-tax->>/>>/", the user went to personal-tax then back to the gov.uk page, then left. This only happened once in that day.

## Length of one-offs vs not-one-offs

The length of a journey will be page urls seperated by ">>" we add one, to give a one-page session a length of one.  

```{r}
# mostly work with this
no_junk <- no_junk %>%
  mutate(journey_length = str_count(PageSequence, ">>") + 1) %>%
  # create a new variable for one-off or not
  mutate(pageseq_type = as_factor(
    if_else(journey_occurrences == 1,
            true = "one-off",
            false = "not-one-off")
    )
         )

```

standard plotting

```{r}
summary(filter(no_junk, pageseq_type == "not-one-off")$journey_length)

summary(filter(no_junk, pageseq_type == "one-off")$journey_length)
```

Tails are really long, can only see outliers...

```{r}
# p <- ggplot(data = no_junk,
#             aes(x = journey_length, y = pageseq_type)) + 
#   geom_violin() + xlim(0, 500)
# 
# p + theme_tufte()
```

And if you want some numbers to quote:

```{r}
br <- seq(0, 500, by = 50)
total_occurrences <- sum(pageseq_sum$journey_length)

ranges <- paste(head(br,-1), br[-1], sep=" - ")
# hist is efficient at binning
freq <- hist(pageseq_sum$journey_length,
              breaks = br, include.lowest = TRUE,
              plot = FALSE)

# for printing
x_bin <- data_frame(range = ranges, frequency = freq$counts) %>%
  mutate(prop = frequency / sum(frequency))

x_bin

rm(br, ranges, freq)
```

Journies less than fifty pages make up almost all journeys.  

```{r}
no_junk %>%
  # filter rows for journeys less than 50 transitions
  filter(journey_length <= 50) %>%
  ggplot(aes(x=journey_length)) + 
  # Adjust the multiplicate bandwidth adjustment; smooth it
  # as discrete
  geom_density(stat = "density", adjust = 3) +
  ggthemes::theme_tufte() +
  xlab("Journey Length") + ylab("Density")
```


## The distribution of journey length and occurrences of a PageSequence

Thus far we've only considered the distributions of these variables. We are more interested in the volume of sessions that are found and how they are distributed amongst these different length journeys. Thus we ask where are most `Occurrences` distributed amongst user journey `PageSequences` of differing lengths?  

A 2d density plot is useful to study the relationship between 2 numeric variables if you have a huge number of points. To avoid overlapping (as in the scatterplot beside), it divides the plot area in a multitude of small fragment and represents the number of points in this fragment.

```{r}
ggplot(no_junk, aes(x = journey_length, y=journey_occurrences) ) +
  geom_hex() +
  theme_bw()
 
```

The popular or well-visited PageSequences draw our attention from lower the y-axis, where the action is at. Let's find an appropriate scale.

```{r}
ggplot(no_junk, aes(x = journey_length, y = journey_occurrences) ) +
  geom_bin2d(bins = 50) + #ylim(0, 1000) +
  theme_bw() +
  scale_y_continuous(trans = "log10") +
  # breaks to read log scale
  scale_x_continuous(trans = "log10", breaks = c(1, 10, 100, 500),
                     minor_breaks = c(1:10, seq(20, 100, 10))
                     ) +
  xlab("Journey Length") + ylab("PageSeq Occurences")
```

```{r}
set.seed(128921)
# Add 2d density estimation
sp <- ggplot(sample_n(no_junk, size = 10000),
             aes(x = journey_length, y = journey_occurrences) ) +
  geom_point(color = "lightgray", alpha = 0.05,
             position = position_jitter(w = 0.1, h = 0.1))
  
sp + theme_bw() +
  scale_y_continuous(trans = "log10") +
  # breaks to read log scale
  scale_x_continuous(trans = "log10", breaks = c(1, 10, 100, 500),
                     minor_breaks = c(1:10, seq(20, 100, 10))
                     ) +
  xlab("Journey Length") + ylab("PageSeq Occurences")
```


Try a distirbution map with raster, these error.

Try datashader, probably get better results.

### Drop One-offs

Let's drop the one-offs, so we can learn about PageSeq with high session volume. Lots pf occurrences.

```{r}
# x defined in useful_df_maker also
no_junk_no_one_offs <- no_junk %>%
  filter(journey_occurrences > 1)

ggplot(no_junk_no_one_offs, aes(x = journey_length, y = journey_occurrences) ) +
  geom_bin2d(bins = 50) + #ylim(0, 1000) +
  theme_bw() +
  scale_y_continuous(trans = "log10") +
  # breaks to read log scale
  scale_x_continuous(trans = "log10", breaks = c(1, 10, 100, 500),
                     minor_breaks = c(1:10, seq(20, 100, 10))
                     ) +
  xlab("Journey Length") + ylab("PageSeq Occurences") + ggtitle("no-one-offs")
```

```{r}
set.seed(128921)
# Add 2d density estimation
sp <- ggplot(sample_n(no_junk_no_one_offs, size = 10000),
             aes(x = journey_length, y = journey_occurrences) ) +
  geom_point(color = "lightgray", alpha = 0.01,
             position = position_jitter(w = 0.1, h = 0))
  
sp + theme_bw() +
  scale_y_continuous(trans = "log10", breaks = c(1, 10, 100, 1e03, 1e04, 1e05)) +
  # breaks to read log scale
  scale_x_continuous(trans = "log10", breaks = c(1, 10, 100, 500),
                     minor_breaks = c(1:10, seq(20, 100, 10))
                     ) +
  xlab("Journey Length") + ylab("PageSeq Occurences") +
  ggtitle("no-one-offs")
```

## Aggregate occurences across journey_length

```{r}
# Using cut
no_junk_jl_factor <- no_junk %>%
  mutate(journey_length_type = cut(journey_length,
      breaks = c(1:11,  101, 500, Inf),
      labels = c(as.character(1:10), "11<=x<100", "101<=x<500", "x>=500"),
      right = FALSE))


```

Barplot it. Needs sense check. Passed.

```{r}

# with  group, in parantheses returns it
(no_junk_jl_factor %>% 
  group_by(journey_length_type) %>% 
  summarise_at(c("journey_occurrences"), sum) -> occurrences_by_length)
```

```{r}
p <- ggplot(occurrences_by_length, aes(journey_length_type, journey_occurrences)) +
  theme_tufte()

# plot with standard form
p + geom_bar(stat = "identity") + theme(axis.text.x=element_text(angle=90,hjust=1)) +
  xlab("Journey length") + ylab("Journey occurences") + 
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE))
```

Self explanatory. What about as a proportion of the total?

```{r}
# as proportion of total
occurrences_by_length <- occurrences_by_length %>%
  mutate(journey_occurrences_prop = journey_occurrences/sum(occurrences_by_length$journey_occurrences))

occurrences_by_length
```

```{r}
# *100 to make %
p <- ggplot(occurrences_by_length, aes(journey_length_type, journey_occurrences_prop*100)) +
  theme_tufte()

# plot with standard form
p + geom_bar(stat = "identity") + theme(axis.text.x=element_text(angle=90,hjust=1)) +
  xlab("Journey length") + ylab("Journey occurences (%)")

```

These 500 journeys are curious, to look at them use:
`filter(no_junk, journey_length == 500) %>% View()` in the console.

## Aggregate occurences across journey_length

But what proportion of these journey length occurences are one-offs?

```{r}
# with  group, in parantheses returns it
(no_junk_jl_factor %>% 
  group_by(journey_length_type, pageseq_type) %>% 
  summarise_at(c("journey_occurrences"), sum) -> occurrences_by_length_w_oneoff_status)

occurrences_by_length_w_oneoff_status <- occurrences_by_length_w_oneoff_status %>%
  mutate(journey_occurrences_prop = journey_occurrences/sum(occurrences_by_length_w_oneoff_status$journey_occurrences))
```


```{r}
# *100 to make %
p <- ggplot(occurrences_by_length_w_oneoff_status,
            aes(journey_length_type, journey_occurrences_prop*100)) +
  theme_tufte()

# dodge bars, rather than stack, i prefer
p + 
  geom_bar(stat = "identity",
           aes(fill = pageseq_type),
           position = "dodge") +
  theme(axis.text.x=element_text(angle = 90,hjust=1)) +
  xlab("Journey length") + ylab("Journey occurences (%)")

```

In one day, as you get to journey lengths or PageSequences of greater than 3, they are "special" enough to be unique, with no other sessions following that route in the same day. Accordingly over lengths of 4 the contribution of occurences of these "one-offs" start to be important, despite by definition, there only being one occurence of them.

```{r}
#What prop do one-offs and not make up?
summarise_at(occurrences_by_length_w_oneoff_status, .vars = "journey_occurrences_prop", .funs = sum)
```

Or the above as numbers

```{r}
# *100 to make %
p <- ggplot(occurrences_by_length_w_oneoff_status,
            aes(journey_length_type, journey_occurrences)) +
  theme_tufte()

# dodge bars, rather than stack, i prefer
p + 
  geom_bar(stat = "identity",
           aes(fill = pageseq_type),
           position = "dodge") +
  theme(axis.text.x=element_text(angle = 90,hjust=1)) +
  xlab("Journey length") + ylab("Journey occurences (%)")  + 
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE))

```

## What happens to the user sessions that only contain `/` or "www.gov.uk/" in their `PageSequence`?

## Session info

```{r}
devtools::session_info()
```

